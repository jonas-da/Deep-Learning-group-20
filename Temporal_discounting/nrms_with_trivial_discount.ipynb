{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NRMS Model with Trivial Discounting\n",
    "### Course: *02456 - Deep Learning*  \n",
    "**Technical University of Denmark (DTU)**  \n",
    "---\n",
    "\n",
    "### 📜 **Context**  \n",
    "- This notebook is created as part of the course *02456 - Deep Learning* at DTU. It demonstrates a news recommender system model using Danish media *Ekstra Bladet*'s dataset to predict user preferences for news articles. The model implementation is inspired by the article [Neural News Recommendation with Multi-Head Self-Attention](https://dl.acm.org/doi/10.1145/3640457.3687164).\n",
    "---\n",
    "\n",
    "### 📝 **Differences from the Original Paper**  \n",
    "- **Adding of Temporal Fetaures**: The published time from the article is taken into account. Relative time deltas are used as inputs for a pre-defined discounting function to calculate discount factors of the news representation.\n",
    "\n",
    "### 🛠️ **What Does This Script Do?**  \n",
    "1. **Model Creation**:  \n",
    "   - Implements an intuitive temporal discounting in tensorflow and integrates it to the nrms model.\n",
    "\n",
    "2. **Training**:  \n",
    "   - Trains the model using data from *Ekstra Bladet's \"2024 RecSys Challenge\"*.\n",
    "\n",
    "3. **Evaluation**:  \n",
    "   - Evaluates the model on a dataset from *Ekstra Bladet's \"2024 RecSys Challenge\"*.\n",
    "---\n",
    "\n",
    "### 💻 **Hardware Setup**  \n",
    "- This notebook has been tested on DTU's HPC and Google Colab Pro using a T4 GPU with 50GB of system RAM.\n",
    "---\n",
    "\n",
    "### 🔗 **References**  \n",
    "1. [Neural News Recommendation with Multi-Head Self-Attention](https://dl.acm.org/doi/10.1145/3640457.3687164)  \n",
    "2. [Extra Bladet's \"2024 RecSys Challenge\"](https://recsys.eb.dk/)\n",
    "3. The main script is inspired by the examples from the organisor from the challenge. The Dataloader, Temporal layer and the integration to the original model is completly self created. \n",
    "---\n",
    "\n",
    "### 🖊️ **Authors**  \n",
    "- Simon Stohrer\n",
    "- Jonas Vincent Ralf Dauscher\n",
    "- Jofre Bonillo Mesegué\n",
    "- Jan Christopher Leisbrock\n",
    "- Emil Kragh Toft\n",
    "\n",
    "### **Reproducibility**\n",
    "The path in the following cell needs to be changed to the location, where your src folder is stored. After the load dataset headline, the path for the file location needs to be changed as well. The fraction is set to a very small value to allow a fast execution of the code, but should be set to 1 for score reproduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Deepl learning/Jan_update/src')  # Add the parent directory to sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "import datetime\n",
    "from typing import List, Dict, Any, Tuple, Optional, Union\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "from ebrec.utils._constants import *\n",
    "\n",
    "from ebrec.utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    "    ebnerd_from_path,\n",
    ")\n",
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers\n",
    "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from ebrec.utils._articles import create_article_id_to_value_mapping\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "from ebrec.utils._python import write_submission_file, rank_predictions_by_score\n",
    "\n",
    "#from ebrec.models.newsrec.dataloader import NewsrecDataLoader \n",
    "from ebrec.models.newsrec.model_config import hparams_nrms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "# List all physical devices\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(\"Available devices:\", physical_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"/content/drive/MyDrive/Deepl learning/Jan_update/ebnerd_data\")\n",
    "#make sure to adjust the path to the data from the ebnerd dataset\n",
    "DATASPLIT = \"ebnerd_small\"\n",
    "DUMP_DIR = Path.joinpath(PATH,\"ebnerd_predictions\")\n",
    "DUMP_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History size can often be a memory bottleneck; if adjusted, the NRMS hyperparameter ```history_size``` must be updated to ensure compatibility and efficient memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORY_SIZE = 20\n",
    "hparams_nrms.history_size = HISTORY_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just want to load the necessary columns\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "]\n",
    "# This notebook is just a simple 'get-started'; we down sample the number of samples to just run quickly through it.\n",
    "FRACTION = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we sample the dataset, just to keep it smaller. We'll split the training data into training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 202\n",
      "Validation samples: 32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>impression_id</th><th>impression_time</th><th>article_id_fixed</th><th>article_ids_clicked</th><th>article_ids_inview</th><th>labels</th></tr><tr><td>u32</td><td>u32</td><td>datetime[μs]</td><td>list[i32]</td><td>list[i64]</td><td>list[i64]</td><td>list[i8]</td></tr></thead><tbody><tr><td>335666</td><td>566421329</td><td>2023-05-23 19:12:35</td><td>[9765438, 9761384, … 9769575]</td><td>[9778168]</td><td>[9778139, 9778277, … 9778277]</td><td>[0, 0, … 0]</td></tr><tr><td>1300067</td><td>123067130</td><td>2023-05-21 18:19:46</td><td>[9768793, 9768566, … 9770452]</td><td>[9775042]</td><td>[9775042, 9769557, … 9775079]</td><td>[1, 0, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 7)\n",
       "┌─────────┬──────────────┬──────────────┬──────────────┬──────────────┬──────────────┬─────────────┐\n",
       "│ user_id ┆ impression_i ┆ impression_t ┆ article_id_f ┆ article_ids_ ┆ article_ids_ ┆ labels      │\n",
       "│ ---     ┆ d            ┆ ime          ┆ ixed         ┆ clicked      ┆ inview       ┆ ---         │\n",
       "│ u32     ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ list[i8]    │\n",
       "│         ┆ u32          ┆ datetime[μs] ┆ list[i32]    ┆ list[i64]    ┆ list[i64]    ┆             │\n",
       "╞═════════╪══════════════╪══════════════╪══════════════╪══════════════╪══════════════╪═════════════╡\n",
       "│ 335666  ┆ 566421329    ┆ 2023-05-23   ┆ [9765438,    ┆ [9778168]    ┆ [9778139,    ┆ [0, 0, … 0] │\n",
       "│         ┆              ┆ 19:12:35     ┆ 9761384, …   ┆              ┆ 9778277, …   ┆             │\n",
       "│         ┆              ┆              ┆ 9769575]     ┆              ┆ 9778277]     ┆             │\n",
       "│ 1300067 ┆ 123067130    ┆ 2023-05-21   ┆ [9768793,    ┆ [9775042]    ┆ [9775042,    ┆ [1, 0, … 0] │\n",
       "│         ┆              ┆ 18:19:46     ┆ 9768566, …   ┆              ┆ 9769557, …   ┆             │\n",
       "│         ┆              ┆              ┆ 9770452]     ┆              ┆ 9775079]     ┆             │\n",
       "└─────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴─────────────┘"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (\n",
    "    ebnerd_from_path(\n",
    "        PATH.joinpath(DATASPLIT, \"train\"),\n",
    "        history_size=HISTORY_SIZE,\n",
    "        padding=0,\n",
    "    )\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=4,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "\n",
    "dt_split = pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL).max() - timedelta(days=1)\n",
    "df_train = df.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL) < dt_split)\n",
    "df_validation = df.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL) >= dt_split)\n",
    "\n",
    "print(f\"Train samples: {df_train.height}\\nValidation samples: {df_validation.height}\")\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set\n",
    "We'll use the validation set, as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = (\n",
    "    ebnerd_from_path(\n",
    "        PATH.joinpath(DATASPLIT, \"validation\"),\n",
    "        history_size=HISTORY_SIZE,\n",
    "        padding=0,\n",
    "    )\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = pl.read_parquet(PATH.joinpath(DATASPLIT+\"/articles.parquet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare temporal features\n",
    "\n",
    "def create_article_time_dict(df_articles: pl.DataFrame) -> Dict[int, datetime]:\n",
    "    \"\"\"Create lookup dictionary for article publishing times\"\"\"\n",
    "    return dict(zip(\n",
    "        df_articles[\"article_id\"].to_list(),\n",
    "        df_articles[\"published_time\"].to_list()\n",
    "    ))\n",
    "article_time_dict = create_article_time_dict(df_articles)\n",
    "\n",
    "def prepare_temporal_features(\n",
    "    df: pl.DataFrame,\n",
    "    article_time_dict: Dict[int, datetime],\n",
    "    inview_col: str\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Add temporal features using vectorized operations\"\"\"\n",
    "\n",
    "    inview_time_col = f\"published_time_{inview_col}\"\n",
    "\n",
    "    return df.with_columns([\n",
    "        pl.col(inview_col).map_elements(\n",
    "            lambda ids: [article_time_dict.get(id) for id in ids],\n",
    "            return_dtype=pl.List(pl.Datetime)\n",
    "        ).alias(inview_time_col)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add temporal features\n",
    "df_train = prepare_temporal_features(\n",
    "    df_train,\n",
    "    article_time_dict,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL\n",
    ")\n",
    "\n",
    "df_validation = prepare_temporal_features(\n",
    "    df_validation,\n",
    "    article_time_dict,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL\n",
    ")\n",
    "\n",
    "df_test = prepare_temporal_features(\n",
    "    df_test,\n",
    "    article_time_dict,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL\n",
    ")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_temporal_differences(\n",
    "    df: pl.DataFrame,\n",
    "    inview_time_col: str\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Compute time differences and exponential discounts\"\"\"\n",
    "\n",
    "    # Add reference date (latest date from inview articles)\n",
    "    df = df.with_columns(\n",
    "        pl.col(inview_time_col)\n",
    "        .map_elements(\n",
    "            lambda dates: max((d for d in dates if d), default=None),\n",
    "            return_dtype=pl.Datetime\n",
    "        )\n",
    "        .alias(\"reference_date\")\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "def calculate_time_difference_seconds(\n",
    "    timestamps: List[Optional[datetime]], \n",
    "    reference_time: datetime\n",
    ") -> List[Optional[float]]:\n",
    "    \"\"\"\n",
    "    Calculate the time difference in seconds between a list of timestamps and a reference time.\n",
    "    \n",
    "    Args:\n",
    "        timestamps: List of timestamps to compare (can contain None)\n",
    "        reference_time: The reference timestamp to compare against\n",
    "        \n",
    "    Returns:\n",
    "        List of time differences in seconds or None if timestamp is None\n",
    "    \"\"\"\n",
    "    return [(reference_time - timestamp).total_seconds() if timestamp else None for timestamp in timestamps]\n",
    "\n",
    "def add_time_difference_column(\n",
    "    df: pl.DataFrame,\n",
    "    timestamp_column: str,\n",
    "    reference_time_column: str,\n",
    "    output_column: str\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a column with time differences in seconds between lists of timestamps and a reference time.\n",
    "    \n",
    "    Args:\n",
    "        df: Input Polars DataFrame\n",
    "        timestamp_column: Name of column containing lists of timestamps\n",
    "        reference_time_column: Name of column containing the reference time\n",
    "        output_column: Name of output column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added time difference column\n",
    "    \"\"\"\n",
    "    df = df.with_columns([\n",
    "        pl.struct([timestamp_column, reference_time_column]).map_elements(\n",
    "            lambda row: calculate_time_difference_seconds(row[timestamp_column], row[reference_time_column]),\n",
    "            return_dtype=pl.List(pl.Float64)\n",
    "        ).alias(output_column)\n",
    "    ])\n",
    "\n",
    "    return df\n",
    "def compute_exponential_discount(deltas: List[Optional[float]]) -> List[Optional[float]]:\n",
    "    \"\"\"\n",
    "    Compute exponential discount based on time deltas.\n",
    "    \n",
    "    Args:\n",
    "        deltas: List of time deltas in seconds\n",
    "        \n",
    "    Returns:\n",
    "        List of discounts\n",
    "    \"\"\"\n",
    "    \n",
    "    max_delta = max((d for d in deltas if d is not None), default=1)\n",
    "    max_delta = max(1, max_delta)  # Ensure max_delta is at least 1 to avoid division by zero\n",
    "    \n",
    "    return [np.exp(-d / (max_delta*4)) if d is not None else None for d in deltas]\n",
    "\n",
    "def add_discount_column(\n",
    "    df: pl.DataFrame,\n",
    "    time_delta_column: str,\n",
    "    output_column: str\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a column with exponential discounts based on time deltas.\n",
    "    \n",
    "    Args:\n",
    "        df: Input Polars DataFrame\n",
    "        time_delta_column: Name of column containing lists of time deltas\n",
    "        output_column: Name of output column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added discount column\n",
    "    \"\"\"\n",
    "    df = df.with_columns([\n",
    "        pl.col(time_delta_column).map_elements(\n",
    "            compute_exponential_discount,\n",
    "            return_dtype=pl.List(pl.Float64)\n",
    "        ).alias(output_column)\n",
    "    ])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_train = compute_temporal_differences(\n",
    "    df_train,\n",
    "    f\"published_time_{DEFAULT_INVIEW_ARTICLES_COL}\"\n",
    ")\n",
    "\n",
    "df_validation = compute_temporal_differences(\n",
    "    df_validation,\n",
    "    f\"published_time_{DEFAULT_INVIEW_ARTICLES_COL}\"\n",
    ")\n",
    "\n",
    "df_test = compute_temporal_differences(\n",
    "    df_test,\n",
    "    f\"published_time_{DEFAULT_INVIEW_ARTICLES_COL}\"\n",
    ")\n",
    "\n",
    "df_train = add_time_difference_column(\n",
    "    df_train,\n",
    "    f\"published_time_{DEFAULT_INVIEW_ARTICLES_COL}\",\n",
    "    \"reference_date\", \n",
    "    \"time_delta\"\n",
    ")\n",
    "\n",
    "df_validation = add_time_difference_column(\n",
    "    df_validation,\n",
    "    f\"published_time_{DEFAULT_INVIEW_ARTICLES_COL}\",\n",
    "    \"reference_date\", \n",
    "    \"time_delta\"\n",
    ")\n",
    "\n",
    "df_test = add_time_difference_column(\n",
    "    df_test,\n",
    "    f\"published_time_{DEFAULT_INVIEW_ARTICLES_COL}\",\n",
    "    \"reference_date\", \n",
    "    \"time_delta\"\n",
    ")\n",
    "\n",
    "df_train = add_discount_column(\n",
    "    df_train,\n",
    "    \"time_delta\",\n",
    "    \"discount_time_delta\"\n",
    ")\n",
    "\n",
    "df_validation = add_discount_column(\n",
    "    df_validation,\n",
    "    \"time_delta\",\n",
    "    \"discount_time_delta\"\n",
    ")\n",
    "\n",
    "df_test = add_discount_column(\n",
    "    df_test,\n",
    "    \"time_delta\",\n",
    "    \"discount_time_delta\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model using HuggingFace's tokenizer and wordembedding\n",
    "In the original implementation, they use the GloVe embeddings and tokenizer. To get going fast, we'll use a multilingual LLM from Hugging Face. \n",
    "Utilizing the tokenizer to tokenize the articles and the word-embedding to init NRMS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "# We'll init the word embeddings using the\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "#\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "# =>\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate the customized dataloaders\n",
    "In the implementations we have disconnected the models and data. Hence, you should built a dataloader that fits your needs.\n",
    "\n",
    "Note, with this ```NRMSDataLoader``` the ```eval_mode=False``` is meant for ```model.model.fit()``` whereas ```eval_mode=True``` is meant for ```model.scorer.predict()```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "from ebrec.utils._articles_behaviors import map_list_article_id_to_value\n",
    "from ebrec.utils._python import (\n",
    "    repeat_by_list_values_from_matrix,\n",
    "    create_lookup_objects,\n",
    ")\n",
    "\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_USER_COL,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NRMSTemporalDataLoader(NewsrecDataLoader):\n",
    "    def transform(self, df: pl.DataFrame) -> pl.DataFrame:\n",
    "        return df.pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.history_column,\n",
    "            mapping=self.lookup_article_index,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        ).pipe(\n",
    "            map_list_article_id_to_value,\n",
    "            behaviors_column=self.inview_col,\n",
    "            mapping=self.lookup_article_index,\n",
    "            fill_nulls=self.unknown_index,\n",
    "            drop_nulls=False,\n",
    "        )\n",
    " \n",
    "    def __getitem__(self, idx) -> tuple[tuple[np.ndarray], np.ndarray]:\n",
    "        \"\"\"\n",
    "        his_input_title:    (samples, history_size, document_dimension)\n",
    "        pred_input_title:   (samples, npratio, document_dimension)\n",
    "        discount_time_delta: (samples, npratio, document_dimension)\n",
    "        batch_y:            (samples, npratio)\n",
    "        \"\"\"\n",
    "        batch_X = self.X[idx * self.batch_size : (idx + 1) * self.batch_size].pipe(\n",
    "            self.transform\n",
    "        )\n",
    "        batch_y = self.y[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        # =>\n",
    "        if self.eval_mode:\n",
    "            repeats = np.array(batch_X[\"n_samples\"])\n",
    "            # =>\n",
    "            batch_y = np.array(batch_y.explode().to_list()).reshape(-1, 1)\n",
    "            # =>\n",
    "            his_input_title = repeat_by_list_values_from_matrix(\n",
    "                batch_X[self.history_column].to_list(),\n",
    "                matrix=self.lookup_article_matrix,\n",
    "                repeats=repeats,\n",
    "            )\n",
    "            # =>\n",
    "            pred_input_title = self.lookup_article_matrix[\n",
    "                batch_X[self.inview_col].explode().to_list()\n",
    "            ]\n",
    " \n",
    "            discount_time_delta = np.array(batch_X[\"discount_time_delta\"].explode().to_list()).reshape(-1, 1, 1)\n",
    "        else:\n",
    "            batch_y = np.array(batch_y.to_list())\n",
    "            his_input_title = self.lookup_article_matrix[\n",
    "                batch_X[self.history_column].to_list()\n",
    "            ]\n",
    "            pred_input_title = self.lookup_article_matrix[\n",
    "                batch_X[self.inview_col].to_list()\n",
    "            ]\n",
    "            pred_input_title = np.squeeze(pred_input_title, axis=2)\n",
    " \n",
    "            discount_time_delta = np.array(batch_X[\"discount_time_delta\"].to_list())\n",
    " \n",
    "        his_input_title = np.squeeze(his_input_title, axis=2)\n",
    "        return (his_input_title, pred_input_title, discount_time_delta), batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataloader = NRMSTemporalDataLoader(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "val_dataloader = NRMSTemporalDataLoader(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate the custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "from ebrec.models.newsrec.layers import AttLayer2, SelfAttention\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    " \n",
    "from tensorflow.keras.layers import Embedding, Input, Dropout, Dense, BatchNormalization\n",
    "from tensorflow.keras.initializers import GlorotUniform\n",
    "from tensorflow.keras.regularizers import l2\n",
    " \n",
    " \n",
    "class NRMSTemporalModel:\n",
    "    \"\"\"NRMS model(Neural News Recommendation with Multi-Head Self-Attention)\n",
    " \n",
    "    Chuhan Wu, Fangzhao Wu, Suyu Ge, Tao Qi, Yongfeng Huang,and Xing Xie, \"Neural News\n",
    "    Recommendation with Multi-Head Self-Attention\" in Proceedings of the 2019 Conference\n",
    "    on Empirical Methods in Natural Language Processing and the 9th International Joint Conference\n",
    "    on Natural Language Processing (EMNLP-IJCNLP)\n",
    " \n",
    "    Attributes:\n",
    "    \"\"\"\n",
    " \n",
    "    def __init__(\n",
    "        self,\n",
    "        hparams: dict,\n",
    "        word2vec_embedding: np.ndarray = None,\n",
    "        word_emb_dim: int = 300,\n",
    "        vocab_size: int = 32000,\n",
    "        seed: int = None,\n",
    "    ):\n",
    "        \"\"\"Initialization steps for NRMS.\"\"\"\n",
    "        self.hparams = hparams\n",
    "        self.seed = seed\n",
    " \n",
    "        # SET SEED:\n",
    "        tf.random.set_seed(seed)\n",
    "        np.random.seed(seed)\n",
    " \n",
    "        # INIT THE WORD-EMBEDDINGS:\n",
    "        if word2vec_embedding is None:\n",
    "            # Xavier Initialization\n",
    "            initializer = GlorotUniform(seed=self.seed)\n",
    "            self.word2vec_embedding = initializer(shape=(vocab_size, word_emb_dim))\n",
    "            # self.word2vec_embedding = np.random.rand(vocab_size, word_emb_dim)\n",
    "        else:\n",
    "            self.word2vec_embedding = word2vec_embedding\n",
    " \n",
    "        # BUILD AND COMPILE MODEL:\n",
    "        self.model, self.scorer = self._build_graph()\n",
    "        data_loss = self._get_loss(self.hparams.loss)\n",
    "        train_optimizer = self._get_opt(\n",
    "            optimizer=self.hparams.optimizer, lr=self.hparams.learning_rate\n",
    "        )\n",
    "        self.model.compile(loss=data_loss, optimizer=train_optimizer)\n",
    " \n",
    "    def _get_loss(self, loss: str):\n",
    "        \"\"\"Make loss function, consists of data loss and regularization loss\n",
    "        Returns:\n",
    "            object: Loss function or loss function name\n",
    "        \"\"\"\n",
    "        if loss == \"cross_entropy_loss\":\n",
    "            data_loss = \"categorical_crossentropy\"\n",
    "        elif loss == \"log_loss\":\n",
    "            data_loss = \"binary_crossentropy\"\n",
    "        else:\n",
    "            raise ValueError(f\"this loss not defined {loss}\")\n",
    "        return data_loss\n",
    " \n",
    "    def _get_opt(self, optimizer: str, lr: float):\n",
    "        \"\"\"Get the optimizer according to configuration. Usually we will use Adam.\n",
    "        Returns:\n",
    "            object: An optimizer.\n",
    "        \"\"\"\n",
    "        # TODO: shouldn't be a string input you should just set the optimizer, to avoid stuff like this:\n",
    "        # => 'WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.'\n",
    "        if optimizer == \"adam\":\n",
    "            train_opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        else:\n",
    "            raise ValueError(f\"this optimizer not defined {optimizer}\")\n",
    "        return train_opt\n",
    " \n",
    "    def _build_graph(self):\n",
    "        \"\"\"Build NRMS model and scorer.\n",
    " \n",
    "        Returns:\n",
    "            object: a model used to train.\n",
    "            object: a model used to evaluate and inference.\n",
    "        \"\"\"\n",
    "        model, scorer = self._build_nrms()\n",
    "        return model, scorer\n",
    " \n",
    "    def _build_userencoder(self, titleencoder):\n",
    "        \"\"\"The main function to create user encoder of NRMS.\n",
    " \n",
    "        Args:\n",
    "            titleencoder (object): the news encoder of NRMS.\n",
    " \n",
    "        Return:\n",
    "            object: the user encoder of NRMS.\n",
    "        \"\"\"\n",
    "        his_input_title = tf.keras.Input(\n",
    "            shape=(self.hparams.history_size, self.hparams.title_size), dtype=\"int32\"\n",
    "        )\n",
    " \n",
    "        click_title_presents = tf.keras.layers.TimeDistributed(titleencoder)(\n",
    "            his_input_title\n",
    "        )\n",
    "        y = SelfAttention(self.hparams.head_num, self.hparams.head_dim, seed=self.seed)(\n",
    "            [click_title_presents] * 3\n",
    "        )\n",
    "        user_present = AttLayer2(self.hparams.attention_hidden_dim, seed=self.seed)(y)\n",
    " \n",
    "        model = tf.keras.Model(his_input_title, user_present, name=\"user_encoder\")\n",
    "        return model\n",
    " \n",
    "    def _build_newsencoder(self):\n",
    "        \"\"\"The main function to create news encoder of NRMS.\n",
    " \n",
    "        Args:\n",
    "            embedding_layer (object): a word embedding layer.\n",
    " \n",
    "        Return:\n",
    "            object: the news encoder of NRMS.\n",
    "        \"\"\"\n",
    "        embedding_layer = tf.keras.layers.Embedding(\n",
    "            self.word2vec_embedding.shape[0],\n",
    "            self.word2vec_embedding.shape[1],\n",
    "            weights=[self.word2vec_embedding],\n",
    "            trainable=True,\n",
    "        )\n",
    "        sequences_input_title = tf.keras.Input(\n",
    "            shape=(self.hparams.title_size,), dtype=\"int32\"\n",
    "        )\n",
    "        embedded_sequences_title = embedding_layer(sequences_input_title)\n",
    " \n",
    "        y = tf.keras.layers.Dropout(self.hparams.dropout)(embedded_sequences_title)\n",
    "        y = SelfAttention(self.hparams.head_num, self.hparams.head_dim, seed=self.seed)(\n",
    "            [y, y, y]\n",
    "        )\n",
    " \n",
    "        # Create configurable Dense layers:\n",
    "        for layer in [400, 400, 400]:\n",
    "            y = tf.keras.layers.Dense(units=layer, activation=\"relu\")(y)\n",
    "            y = tf.keras.layers.BatchNormalization()(y)\n",
    "            y = tf.keras.layers.Dropout(self.hparams.dropout)(y)\n",
    " \n",
    "        y = tf.keras.layers.Dropout(self.hparams.dropout)(y)\n",
    "        pred_title = AttLayer2(self.hparams.attention_hidden_dim, seed=self.seed)(y)\n",
    " \n",
    "        model = tf.keras.Model(sequences_input_title, pred_title, name=\"news_encoder\")\n",
    "        return model\n",
    " \n",
    "    def _build_nrms(self):\n",
    "        \"\"\"The main function to create NRMS's logic. The core of NRMS\n",
    "        is a user encoder and a news encoder.\n",
    " \n",
    "        Returns:\n",
    "            object: a model used to train.\n",
    "            object: a model used to evaluate and inference.\n",
    "        \"\"\"\n",
    " \n",
    "        his_input_title = tf.keras.Input(\n",
    "            shape=(self.hparams.history_size, self.hparams.title_size),\n",
    "            dtype=\"int32\",\n",
    "        )\n",
    "        pred_input_title = tf.keras.Input(\n",
    "            # shape = (hparams.npratio + 1, hparams.title_size)\n",
    "            shape=(None, self.hparams.title_size),\n",
    "            dtype=\"int32\",\n",
    "        )\n",
    "        pred_input_title_one = tf.keras.Input(\n",
    "            shape=(\n",
    "                1,\n",
    "                self.hparams.title_size,\n",
    "            ),\n",
    "            dtype=\"int32\",\n",
    "        )\n",
    " \n",
    "        discount_time_delta = tf.keras.Input(\n",
    "            shape=(None, 1), dtype=\"float32\"\n",
    "        )\n",
    " \n",
    "        discount_time_delta_one = tf.keras.Input(\n",
    "            shape=(1, 1), dtype=\"float32\"\n",
    "        )\n",
    " \n",
    "        pred_title_one_reshape = tf.keras.layers.Reshape((self.hparams.title_size,))(\n",
    "            pred_input_title_one\n",
    "        )\n",
    "        titleencoder = self._build_newsencoder()\n",
    "        self.userencoder = self._build_userencoder(titleencoder)\n",
    "        self.newsencoder = titleencoder\n",
    " \n",
    "        user_present = self.userencoder(his_input_title)\n",
    "        news_present = tf.keras.layers.TimeDistributed(self.newsencoder)(\n",
    "            pred_input_title\n",
    "        )\n",
    "        news_present = tf.keras.layers.Multiply()( [news_present, discount_time_delta])\n",
    "       \n",
    "        news_present_one = self.newsencoder(pred_title_one_reshape)\n",
    "        news_present_one = tf.keras.layers.Multiply()([news_present_one, discount_time_delta_one])\n",
    " \n",
    "        preds = tf.keras.layers.Dot(axes=-1)([news_present, user_present])\n",
    "        preds = tf.keras.layers.Activation(activation=\"softmax\")(preds)\n",
    " \n",
    "        pred_one = tf.keras.layers.Dot(axes=-1)([news_present_one, user_present])\n",
    "        pred_one = tf.keras.layers.Activation(activation=\"sigmoid\")(pred_one)\n",
    " \n",
    "        model = tf.keras.Model([his_input_title, pred_input_title, discount_time_delta], preds)\n",
    "        scorer = tf.keras.Model([his_input_title, pred_input_title_one, discount_time_delta_one], pred_one)\n",
    " \n",
    "        return model, scorer\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "# List all physical devices\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(\"Available devices:\", physical_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate the NRMS-model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janle\\Desktop\\Master_local\\Data_storage\\Deep_learning\\ebnerd_data\\ebnerd_predictions\\state_dict\\NRMSTemporalModel\\mini.weights.h5\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janle\\anaconda3\\envs\\deep_learning\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "c:\\Users\\janle\\anaconda3\\envs\\deep_learning\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_38', 'keras_tensor_39', 'keras_tensor_41']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n",
      "c:\\Users\\janle\\anaconda3\\envs\\deep_learning\\Lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 192001536 elements. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - AUC: 0.4890 - loss: 3.6007\n",
      "Epoch 1: saving model to C:\\Users\\janle\\Desktop\\Master_local\\Data_storage\\Deep_learning\\ebnerd_data\\ebnerd_predictions\\state_dict\\NRMSTemporalModel\\mini.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 3s/step - AUC: 0.4906 - loss: 3.5912 - val_AUC: 0.4855 - val_loss: 2.3757 - learning_rate: 1.0000e-04\n",
      "Epoch 2/4\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - AUC: 0.7048 - loss: 1.9654\n",
      "Epoch 2: saving model to C:\\Users\\janle\\Desktop\\Master_local\\Data_storage\\Deep_learning\\ebnerd_data\\ebnerd_predictions\\state_dict\\NRMSTemporalModel\\mini.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 2s/step - AUC: 0.7018 - loss: 1.9873 - val_AUC: 0.5070 - val_loss: 5.0848 - learning_rate: 1.0000e-04\n",
      "Epoch 3/4\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - AUC: 0.7120 - loss: 1.9289\n",
      "Epoch 3: saving model to C:\\Users\\janle\\Desktop\\Master_local\\Data_storage\\Deep_learning\\ebnerd_data\\ebnerd_predictions\\state_dict\\NRMSTemporalModel\\mini.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 2s/step - AUC: 0.7119 - loss: 1.9270 - val_AUC: 0.4901 - val_loss: 4.7797 - learning_rate: 1.0000e-04\n",
      "Epoch 4/4\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - AUC: 0.7395 - loss: 1.7748\n",
      "Epoch 4: saving model to C:\\Users\\janle\\Desktop\\Master_local\\Data_storage\\Deep_learning\\ebnerd_data\\ebnerd_predictions\\state_dict\\NRMSTemporalModel\\mini.weights.h5\n",
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - AUC: 0.7423 - loss: 1.7608 - val_AUC: 0.5055 - val_loss: 4.9988 - learning_rate: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "model = NRMSTemporalModel(\n",
    "    hparams=hparams_nrms,\n",
    "    word2vec_embedding=word2vec_embedding,\n",
    "    seed=42,\n",
    ")\n",
    "model.model.compile(\n",
    "    optimizer=model.model.optimizer,\n",
    "    loss=model.model.loss,\n",
    "    metrics=[\"AUC\"],\n",
    ")\n",
    "\n",
    "MODEL_NAME = model.__class__.__name__\n",
    "MODEL_WEIGHTS = DUMP_DIR.joinpath(f\"state_dict/{MODEL_NAME}/mini.weights.h5\")\n",
    "LOG_DIR = DUMP_DIR.joinpath(f\"runs/{MODEL_NAME}\")\n",
    "print(MODEL_WEIGHTS)\n",
    "### Callbacks\n",
    "#We will add some callbacks to model training.\n",
    "# Tensorboard:\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "#    log_dir=LOG_DIR,\n",
    "#    histogram_freq=1,\n",
    "#)\n",
    "\n",
    "# Earlystopping:\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_AUC\",\n",
    "    mode=\"max\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# ModelCheckpoint:\n",
    "modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=MODEL_WEIGHTS,\n",
    "    monitor=\"val_AUC\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=False,\n",
    "    save_weights_only=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Learning rate scheduler:\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_AUC\",\n",
    "    mode=\"max\",\n",
    "    factor=0.2,\n",
    "    patience=2,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, modelcheckpoint, lr_scheduler]#tensorboard_callback\n",
    "USE_CALLBACKS = True\n",
    "EPOCHS = 4\n",
    "\n",
    "hist = model.model.fit(\n",
    "    train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks if USE_CALLBACKS else [],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CALLBACKS:\n",
    "    _ = model.model.load_weights(filepath=MODEL_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example how to compute some metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_TEST = 16\n",
    "\n",
    "test_dataloader = NRMSTemporalDataLoader(\n",
    "    behaviors=df_test,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=BATCH_SIZE_TEST,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janle\\anaconda3\\envs\\deep_learning\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_38', 'keras_tensor_40', 'keras_tensor_42']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2s/step\n"
     ]
    }
   ],
   "source": [
    "pred_test = model.scorer.predict(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the predictions to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>impression_id</th><th>impression_time</th><th>article_id_fixed</th><th>article_ids_clicked</th><th>article_ids_inview</th><th>labels</th><th>published_time_article_ids_inview</th><th>reference_date</th><th>time_delta</th><th>discount_time_delta</th><th>scores</th></tr><tr><td>u32</td><td>u32</td><td>datetime[μs]</td><td>list[i32]</td><td>list[i32]</td><td>list[i32]</td><td>list[i8]</td><td>list[datetime[μs]]</td><td>datetime[μs]</td><td>list[f64]</td><td>list[f64]</td><td>list[f32]</td></tr></thead><tbody><tr><td>2530015</td><td>434585758</td><td>2023-05-26 07:20:38</td><td>[9779383, 9779269, … 9780181]</td><td>[9778944]</td><td>[9781598, 9781624, … 9186608]</td><td>[0, 0, … 0]</td><td>[2023-05-25 20:53:00, 2023-05-25 18:22:12, … 2022-04-21 05:41:48]</td><td>2023-05-26 03:12:18</td><td>[22758.0, 31806.0, … 3.455103e7]</td><td>[0.999835, 0.99977, … 0.778801]</td><td>[0.071588, 0.172548, … 0.00758]</td></tr><tr><td>1227139</td><td>7074582</td><td>2023-05-30 20:21:31</td><td>[9780096, 9780096, … 9780181]</td><td>[9788149]</td><td>[9788149, 9780702, … 9506503]</td><td>[1, 0, … 0]</td><td>[2023-05-30 19:48:09, 2023-05-30 07:43:11, … 2022-11-11 08:55:58]</td><td>2023-05-30 19:48:09</td><td>[0.0, 43498.0, … 1.7319131e7]</td><td>[1.0, 0.999372, … 0.778801]</td><td>[0.002128, 0.944029, … 0.018472]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 12)\n",
       "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ user_id ┆ impression ┆ impressio ┆ article_i ┆ … ┆ reference ┆ time_delt ┆ discount_ ┆ scores    │\n",
       "│ ---     ┆ _id        ┆ n_time    ┆ d_fixed   ┆   ┆ _date     ┆ a         ┆ time_delt ┆ ---       │\n",
       "│ u32     ┆ ---        ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ a         ┆ list[f32] │\n",
       "│         ┆ u32        ┆ datetime[ ┆ list[i32] ┆   ┆ datetime[ ┆ list[f64] ┆ ---       ┆           │\n",
       "│         ┆            ┆ μs]       ┆           ┆   ┆ μs]       ┆           ┆ list[f64] ┆           │\n",
       "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 2530015 ┆ 434585758  ┆ 2023-05-2 ┆ [9779383, ┆ … ┆ 2023-05-2 ┆ [22758.0, ┆ [0.999835 ┆ [0.071588 │\n",
       "│         ┆            ┆ 6         ┆ 9779269,  ┆   ┆ 6         ┆ 31806.0,  ┆ ,         ┆ ,         │\n",
       "│         ┆            ┆ 07:20:38  ┆ …         ┆   ┆ 03:12:18  ┆ … 3.45510 ┆ 0.99977,  ┆ 0.172548, │\n",
       "│         ┆            ┆           ┆ 9780181]  ┆   ┆           ┆ 3e…       ┆ …         ┆ …         │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆ 0.778801… ┆ 0.00758…  │\n",
       "│ 1227139 ┆ 7074582    ┆ 2023-05-3 ┆ [9780096, ┆ … ┆ 2023-05-3 ┆ [0.0,     ┆ [1.0,     ┆ [0.002128 │\n",
       "│         ┆            ┆ 0         ┆ 9780096,  ┆   ┆ 0         ┆ 43498.0,  ┆ 0.999372, ┆ ,         │\n",
       "│         ┆            ┆ 20:21:31  ┆ …         ┆   ┆ 19:48:09  ┆ … 1.73191 ┆ …         ┆ 0.944029, │\n",
       "│         ┆            ┆           ┆ 9780181]  ┆   ┆           ┆ 31e7]     ┆ 0.778801] ┆ …         │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆           ┆ 0.01847…  │\n",
       "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = add_prediction_scores(df_test, pred_test)\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AUC: 100%|███████████████████████████████████| 244/244 [00:00<00:00, 615.02it/s]\n",
      "AUC: 100%|████████████████████████████████████████████| 244/244 [00:00<?, ?it/s]\n",
      "AUC: 100%|█████████████████████████████████| 244/244 [00:00<00:00, 10421.69it/s]\n",
      "AUC: 100%|█████████████████████████████████| 244/244 [00:00<00:00, 17789.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MetricEvaluator class>: \n",
       " {\n",
       "    \"auc\": 0.5380245008302906,\n",
       "    \"mrr\": 0.34388649184913933,\n",
       "    \"ndcg@5\": 0.37738480858637524,\n",
       "    \"ndcg@10\": 0.4589275216023012\n",
       "}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = MetricEvaluator(\n",
    "    labels=df_test[\"labels\"].to_list(),\n",
    "    predictions=df_test[\"scores\"].to_list(),\n",
    "    metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    ")\n",
    "metrics.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janle\\AppData\\Local\\Temp\\ipykernel_29800\\721020885.py:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  df_test = df_test.with_columns(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>impression_id</th><th>impression_time</th><th>article_id_fixed</th><th>article_ids_clicked</th><th>article_ids_inview</th><th>labels</th><th>published_time_article_ids_inview</th><th>reference_date</th><th>time_delta</th><th>discount_time_delta</th><th>scores</th><th>ranked_scores</th></tr><tr><td>u32</td><td>u32</td><td>datetime[μs]</td><td>list[i32]</td><td>list[i32]</td><td>list[i32]</td><td>list[i8]</td><td>list[datetime[μs]]</td><td>datetime[μs]</td><td>list[f64]</td><td>list[f64]</td><td>list[f32]</td><td>list[i64]</td></tr></thead><tbody><tr><td>2530015</td><td>434585758</td><td>2023-05-26 07:20:38</td><td>[9779383, 9779269, … 9780181]</td><td>[9778944]</td><td>[9781598, 9781624, … 9186608]</td><td>[0, 0, … 0]</td><td>[2023-05-25 20:53:00, 2023-05-25 18:22:12, … 2022-04-21 05:41:48]</td><td>2023-05-26 03:12:18</td><td>[22758.0, 31806.0, … 3.455103e7]</td><td>[0.999835, 0.99977, … 0.778801]</td><td>[0.071588, 0.172548, … 0.00758]</td><td>[20, 19, … 23]</td></tr><tr><td>1227139</td><td>7074582</td><td>2023-05-30 20:21:31</td><td>[9780096, 9780096, … 9780181]</td><td>[9788149]</td><td>[9788149, 9780702, … 9506503]</td><td>[1, 0, … 0]</td><td>[2023-05-30 19:48:09, 2023-05-30 07:43:11, … 2022-11-11 08:55:58]</td><td>2023-05-30 19:48:09</td><td>[0.0, 43498.0, … 1.7319131e7]</td><td>[1.0, 0.999372, … 0.778801]</td><td>[0.002128, 0.944029, … 0.018472]</td><td>[5, 1, … 3]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 13)\n",
       "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ user_id ┆ impression ┆ impressio ┆ article_i ┆ … ┆ time_delt ┆ discount_ ┆ scores    ┆ ranked_sc │\n",
       "│ ---     ┆ _id        ┆ n_time    ┆ d_fixed   ┆   ┆ a         ┆ time_delt ┆ ---       ┆ ores      │\n",
       "│ u32     ┆ ---        ┆ ---       ┆ ---       ┆   ┆ ---       ┆ a         ┆ list[f32] ┆ ---       │\n",
       "│         ┆ u32        ┆ datetime[ ┆ list[i32] ┆   ┆ list[f64] ┆ ---       ┆           ┆ list[i64] │\n",
       "│         ┆            ┆ μs]       ┆           ┆   ┆           ┆ list[f64] ┆           ┆           │\n",
       "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 2530015 ┆ 434585758  ┆ 2023-05-2 ┆ [9779383, ┆ … ┆ [22758.0, ┆ [0.999835 ┆ [0.071588 ┆ [20, 19,  │\n",
       "│         ┆            ┆ 6         ┆ 9779269,  ┆   ┆ 31806.0,  ┆ ,         ┆ ,         ┆ … 23]     │\n",
       "│         ┆            ┆ 07:20:38  ┆ …         ┆   ┆ … 3.45510 ┆ 0.99977,  ┆ 0.172548, ┆           │\n",
       "│         ┆            ┆           ┆ 9780181]  ┆   ┆ 3e…       ┆ …         ┆ …         ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆ 0.778801… ┆ 0.00758…  ┆           │\n",
       "│ 1227139 ┆ 7074582    ┆ 2023-05-3 ┆ [9780096, ┆ … ┆ [0.0,     ┆ [1.0,     ┆ [0.002128 ┆ [5, 1, …  │\n",
       "│         ┆            ┆ 0         ┆ 9780096,  ┆   ┆ 43498.0,  ┆ 0.999372, ┆ ,         ┆ 3]        │\n",
       "│         ┆            ┆ 20:21:31  ┆ …         ┆   ┆ … 1.73191 ┆ …         ┆ 0.944029, ┆           │\n",
       "│         ┆            ┆           ┆ 9780181]  ┆   ┆ 31e7]     ┆ 0.778801] ┆ …         ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆ 0.01847…  ┆           │\n",
       "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_test.with_columns(\n",
    "    pl.col(\"scores\")\n",
    "    .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "    .alias(\"ranked_scores\")\n",
    ")\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is using the validation, simply add the testset to your flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "244it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping C:\\Users\\janle\\Desktop\\Master_local\\Data_storage\\Deep_learning\\ebnerd_data\\ebnerd_predictions\\predictions.txt to C:\\Users\\janle\\Desktop\\Master_local\\Data_storage\\Deep_learning\\ebnerd_data\\ebnerd_predictions\\ebnerd_small_predictions-NRMSTemporalModel.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_submission_file(\n",
    "    impression_ids=df_test[DEFAULT_IMPRESSION_ID_COL],\n",
    "    prediction_scores=df_test[\"ranked_scores\"],\n",
    "    path=DUMP_DIR.joinpath(\"predictions.txt\"),\n",
    "    filename_zip=f\"{DATASPLIT}_predictions-{MODEL_NAME}.zip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
