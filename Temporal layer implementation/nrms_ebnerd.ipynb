{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started\n",
    "\n",
    "In this notebook, we illustrate how to use the Neural News Recommendation with Multi-Head Self-Attention ([NRMS](https://aclanthology.org/D19-1671/)). The implementation is taken from the [recommenders](https://github.com/recommenders-team/recommenders) repository. We have simply stripped the model to keep it cleaner.\n",
    "\n",
    "We use a small dataset, which is downloaded from [recsys.eb.dk](https://recsys.eb.dk/). All the datasets are stored in the folder path ```~/ebnerd_data/*```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/janle/Desktop/Master_local/3/Deep Learning/deep-learning/Temporal_external/src')  # Add the parent directory to sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "import datetime\n",
    "from typing import List, Dict, Any, Tuple, Optional, Union\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "from ebrec.utils._constants import *\n",
    "\n",
    "from ebrec.utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    "    ebnerd_from_path,\n",
    ")\n",
    "from ebrec.evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from ebrec.utils._articles import convert_text2encoding_with_transformers\n",
    "from ebrec.utils._polars import concat_str_columns, slice_join_dataframes\n",
    "from ebrec.utils._articles import create_article_id_to_value_mapping\n",
    "from ebrec.utils._nlp import get_transformers_word_embeddings\n",
    "from ebrec.utils._python import write_submission_file, rank_predictions_by_score\n",
    "\n",
    "from ebrec.models.newsrec.dataloader import NewsrecDataLoader, NRMSTemporalDataLoader\n",
    "from ebrec.models.newsrec.model_config import hparams_nrms\n",
    "from ebrec.models.newsrec import NRMSTemporalModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "# List all physical devices\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(\"Available devices:\", physical_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate labels\n",
    "We sample a few just to get started. For testset we just make up a dummy column with 0 and 1 - this is not the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path(\"C:/Users/janle/Desktop/Master_local/Data_storage/Deep_learning/ebnerd_data\")\n",
    "#\n",
    "DATASPLIT = \"ebnerd_small\"\n",
    "DUMP_DIR = Path.joinpath(PATH,\"ebnerd_predictions\")\n",
    "DUMP_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/janle/Desktop/Master_local/Data_storage/Deep_learning/ebnerd_data/ebnerd_predictions')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DUMP_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History size can often be a memory bottleneck; if adjusted, the NRMS hyperparameter ```history_size``` must be updated to ensure compatibility and efficient memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORY_SIZE = 20\n",
    "hparams_nrms.history_size = HISTORY_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We just want to load the necessary columns\n",
    "COLUMNS = [\n",
    "    DEFAULT_USER_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "]\n",
    "# This notebook is just a simple 'get-started'; we down sample the number of samples to just run quickly through it.\n",
    "FRACTION = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we sample the dataset, just to keep it smaller. We'll split the training data into training and validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 20147\n",
      "Validation samples: 3280\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>impression_id</th><th>impression_time</th><th>article_id_fixed</th><th>article_ids_clicked</th><th>article_ids_inview</th><th>labels</th></tr><tr><td>u32</td><td>u32</td><td>datetime[μs]</td><td>list[i32]</td><td>list[i64]</td><td>list[i64]</td><td>list[i8]</td></tr></thead><tbody><tr><td>568703</td><td>269413707</td><td>2023-05-20 07:53:39</td><td>[9768377, 9768387, … 9769893]</td><td>[9772601]</td><td>[9773306, 9773306, … 9772629]</td><td>[0, 0, … 0]</td></tr><tr><td>1333156</td><td>328878737</td><td>2023-05-21 06:22:19</td><td>[9770178, 9770328, … 9769457]</td><td>[8054212]</td><td>[9569756, 9773857, … 9569756]</td><td>[0, 0, … 0]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 7)\n",
       "┌─────────┬──────────────┬──────────────┬──────────────┬──────────────┬──────────────┬─────────────┐\n",
       "│ user_id ┆ impression_i ┆ impression_t ┆ article_id_f ┆ article_ids_ ┆ article_ids_ ┆ labels      │\n",
       "│ ---     ┆ d            ┆ ime          ┆ ixed         ┆ clicked      ┆ inview       ┆ ---         │\n",
       "│ u32     ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ ---          ┆ list[i8]    │\n",
       "│         ┆ u32          ┆ datetime[μs] ┆ list[i32]    ┆ list[i64]    ┆ list[i64]    ┆             │\n",
       "╞═════════╪══════════════╪══════════════╪══════════════╪══════════════╪══════════════╪═════════════╡\n",
       "│ 568703  ┆ 269413707    ┆ 2023-05-20   ┆ [9768377,    ┆ [9772601]    ┆ [9773306,    ┆ [0, 0, … 0] │\n",
       "│         ┆              ┆ 07:53:39     ┆ 9768387, …   ┆              ┆ 9773306, …   ┆             │\n",
       "│         ┆              ┆              ┆ 9769893]     ┆              ┆ 9772629]     ┆             │\n",
       "│ 1333156 ┆ 328878737    ┆ 2023-05-21   ┆ [9770178,    ┆ [8054212]    ┆ [9569756,    ┆ [0, 0, … 0] │\n",
       "│         ┆              ┆ 06:22:19     ┆ 9770328, …   ┆              ┆ 9773857, …   ┆             │\n",
       "│         ┆              ┆              ┆ 9769457]     ┆              ┆ 9569756]     ┆             │\n",
       "└─────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴─────────────┘"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = (\n",
    "    ebnerd_from_path(\n",
    "        PATH.joinpath(DATASPLIT, \"train\"),\n",
    "        history_size=HISTORY_SIZE,\n",
    "        padding=0,\n",
    "    )\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=4,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=123,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")\n",
    "\n",
    "dt_split = pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL).max() - timedelta(days=1)\n",
    "df_train = df.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL) < dt_split)\n",
    "df_validation = df.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL) >= dt_split)\n",
    "\n",
    "print(f\"Train samples: {df_train.height}\\nValidation samples: {df_validation.height}\")\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set\n",
    "We'll use the validation set, as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = (\n",
    "    ebnerd_from_path(\n",
    "        PATH.joinpath(DATASPLIT, \"validation\"),\n",
    "        history_size=HISTORY_SIZE,\n",
    "        padding=0,\n",
    "    )\n",
    "    .select(COLUMNS)\n",
    "    .pipe(create_binary_labels_column)\n",
    "    .sample(fraction=FRACTION)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 21)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>article_id</th><th>title</th><th>subtitle</th><th>last_modified_time</th><th>premium</th><th>body</th><th>published_time</th><th>image_ids</th><th>article_type</th><th>url</th><th>ner_clusters</th><th>entity_groups</th><th>topics</th><th>category</th><th>subcategory</th><th>category_str</th><th>total_inviews</th><th>total_pageviews</th><th>total_read_time</th><th>sentiment_score</th><th>sentiment_label</th></tr><tr><td>i32</td><td>str</td><td>str</td><td>datetime[μs]</td><td>bool</td><td>str</td><td>datetime[μs]</td><td>list[i64]</td><td>str</td><td>str</td><td>list[str]</td><td>list[str]</td><td>list[str]</td><td>i16</td><td>list[i16]</td><td>str</td><td>i32</td><td>i32</td><td>f32</td><td>f32</td><td>str</td></tr></thead><tbody><tr><td>3001353</td><td>&quot;Natascha var ikke den første&quot;</td><td>&quot;Politiet frygter nu, at Natasc…</td><td>2023-06-29 06:20:33</td><td>false</td><td>&quot;Sagen om den østriske Natascha…</td><td>2006-08-31 08:06:45</td><td>[3150850]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/krimi/…</td><td>[]</td><td>[]</td><td>[&quot;Kriminalitet&quot;, &quot;Personfarlig kriminalitet&quot;]</td><td>140</td><td>[]</td><td>&quot;krimi&quot;</td><td>null</td><td>null</td><td>null</td><td>0.9955</td><td>&quot;Negative&quot;</td></tr><tr><td>3003065</td><td>&quot;Kun Star Wars tjente mere&quot;</td><td>&quot;Biografgængerne strømmer ind f…</td><td>2023-06-29 06:20:35</td><td>false</td><td>&quot;Vatikanet har opfordret til at…</td><td>2006-05-21 16:57:00</td><td>[3006712]</td><td>&quot;article_default&quot;</td><td>&quot;https://ekstrabladet.dk/underh…</td><td>[]</td><td>[]</td><td>[&quot;Underholdning&quot;, &quot;Film og tv&quot;, &quot;Økonomi&quot;]</td><td>414</td><td>[433, 434]</td><td>&quot;underholdning&quot;</td><td>null</td><td>null</td><td>null</td><td>0.846</td><td>&quot;Positive&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 21)\n",
       "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
       "│ article_i ┆ title     ┆ subtitle  ┆ last_modi ┆ … ┆ total_pag ┆ total_rea ┆ sentiment ┆ sentimen │\n",
       "│ d         ┆ ---       ┆ ---       ┆ fied_time ┆   ┆ eviews    ┆ d_time    ┆ _score    ┆ t_label  │\n",
       "│ ---       ┆ str       ┆ str       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ ---       ┆ ---      │\n",
       "│ i32       ┆           ┆           ┆ datetime[ ┆   ┆ i32       ┆ f32       ┆ f32       ┆ str      │\n",
       "│           ┆           ┆           ┆ μs]       ┆   ┆           ┆           ┆           ┆          │\n",
       "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
       "│ 3001353   ┆ Natascha  ┆ Politiet  ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.9955    ┆ Negative │\n",
       "│           ┆ var ikke  ┆ frygter   ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ den       ┆ nu, at    ┆ 06:20:33  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ første    ┆ Natasc…   ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "│ 3003065   ┆ Kun Star  ┆ Biografgæ ┆ 2023-06-2 ┆ … ┆ null      ┆ null      ┆ 0.846     ┆ Positive │\n",
       "│           ┆ Wars      ┆ ngerne    ┆ 9         ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ tjente    ┆ strømmer  ┆ 06:20:35  ┆   ┆           ┆           ┆           ┆          │\n",
       "│           ┆ mere      ┆ ind f…    ┆           ┆   ┆           ┆           ┆           ┆          │\n",
       "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles = pl.read_parquet(PATH.joinpath(DATASPLIT+\"/articles.parquet\"))\n",
    "df_articles.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare temporal features\n",
    "\n",
    "def create_article_time_dict(df_articles: pl.DataFrame) -> Dict[int, datetime]:\n",
    "    \"\"\"Create lookup dictionary for article publishing times\"\"\"\n",
    "    return dict(zip(\n",
    "        df_articles[\"article_id\"].to_list(),\n",
    "        df_articles[\"published_time\"].to_list()\n",
    "    ))\n",
    "article_time_dict = create_article_time_dict(df_articles)\n",
    "\n",
    "def prepare_temporal_features(\n",
    "    df: pl.DataFrame,\n",
    "    article_time_dict: Dict[int, datetime],\n",
    "    inview_col: str\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Add temporal features using vectorized operations\"\"\"\n",
    "\n",
    "    inview_time_col = f\"published_time_{inview_col}\"\n",
    "\n",
    "    return df.with_columns([\n",
    "        pl.col(inview_col).map_elements(\n",
    "            lambda ids: [article_time_dict.get(id) for id in ids],\n",
    "            return_dtype=pl.List(pl.Datetime)\n",
    "        ).alias(inview_time_col)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add temporal features\n",
    "df_train = prepare_temporal_features(\n",
    "    df_train,\n",
    "    article_time_dict,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL\n",
    ")\n",
    "\n",
    "df_validation = prepare_temporal_features(\n",
    "    df_validation,\n",
    "    article_time_dict,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL\n",
    ")\n",
    "\n",
    "df_test = prepare_temporal_features(\n",
    "    df_test,\n",
    "    article_time_dict,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL\n",
    ")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_temporal_differences(\n",
    "    df: pl.DataFrame,\n",
    "    inview_time_col: str\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"Compute time differences and exponential discounts\"\"\"\n",
    "\n",
    "    # Add reference date (latest date from inview articles)\n",
    "    df = df.with_columns(\n",
    "        pl.col(inview_time_col)\n",
    "        .map_elements(\n",
    "            lambda dates: max((d for d in dates if d), default=None),\n",
    "            return_dtype=pl.Datetime\n",
    "        )\n",
    "        .alias(\"reference_date\")\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "def calculate_time_difference_seconds(\n",
    "    timestamps: List[Optional[datetime]], \n",
    "    reference_time: datetime\n",
    ") -> List[Optional[float]]:\n",
    "    \"\"\"\n",
    "    Calculate the time difference in seconds between a list of timestamps and a reference time.\n",
    "    \n",
    "    Args:\n",
    "        timestamps: List of timestamps to compare (can contain None)\n",
    "        reference_time: The reference timestamp to compare against\n",
    "        \n",
    "    Returns:\n",
    "        List of time differences in seconds or None if timestamp is None\n",
    "    \"\"\"\n",
    "    return [(reference_time - timestamp).total_seconds() if timestamp else None for timestamp in timestamps]\n",
    "\n",
    "def add_time_difference_column(\n",
    "    df: pl.DataFrame,\n",
    "    timestamp_column: str,\n",
    "    reference_time_column: str,\n",
    "    output_column: str\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a column with time differences in seconds between lists of timestamps and a reference time.\n",
    "    \n",
    "    Args:\n",
    "        df: Input Polars DataFrame\n",
    "        timestamp_column: Name of column containing lists of timestamps\n",
    "        reference_time_column: Name of column containing the reference time\n",
    "        output_column: Name of output column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added time difference column\n",
    "    \"\"\"\n",
    "    df = df.with_columns([\n",
    "        pl.struct([timestamp_column, reference_time_column]).map_elements(\n",
    "            lambda row: calculate_time_difference_seconds(row[timestamp_column], row[reference_time_column]),\n",
    "            return_dtype=pl.List(pl.Float64)\n",
    "        ).alias(output_column)\n",
    "    ])\n",
    "\n",
    "    return df\n",
    "def compute_exponential_discount(deltas: List[Optional[float]]) -> List[Optional[float]]:\n",
    "    \"\"\"\n",
    "    Compute exponential discount based on time deltas.\n",
    "    \n",
    "    Args:\n",
    "        deltas: List of time deltas in seconds\n",
    "        \n",
    "    Returns:\n",
    "        List of discounts\n",
    "    \"\"\"\n",
    "    \n",
    "    max_delta = max((d for d in deltas if d is not None), default=1)\n",
    "    max_delta = max(1, max_delta)  # Ensure max_delta is at least 1 to avoid division by zero\n",
    "    \n",
    "    return [np.exp(-d / (max_delta*4)) if d is not None else None for d in deltas]\n",
    "\n",
    "def add_discount_column(\n",
    "    df: pl.DataFrame,\n",
    "    time_delta_column: str,\n",
    "    output_column: str\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a column with exponential discounts based on time deltas.\n",
    "    \n",
    "    Args:\n",
    "        df: Input Polars DataFrame\n",
    "        time_delta_column: Name of column containing lists of time deltas\n",
    "        output_column: Name of output column\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with added discount column\n",
    "    \"\"\"\n",
    "    df = df.with_columns([\n",
    "        pl.col(time_delta_column).map_elements(\n",
    "            compute_exponential_discount,\n",
    "            return_dtype=pl.List(pl.Float64)\n",
    "        ).alias(output_column)\n",
    "    ])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>impression_id</th><th>impression_time</th><th>article_id_fixed</th><th>article_ids_clicked</th><th>article_ids_inview</th><th>labels</th><th>published_time_article_ids_inview</th><th>reference_date</th><th>time_delta</th><th>discount_time_delta</th></tr><tr><td>u32</td><td>u32</td><td>datetime[μs]</td><td>list[i32]</td><td>list[i64]</td><td>list[i64]</td><td>list[i8]</td><td>list[datetime[μs]]</td><td>datetime[μs]</td><td>list[f64]</td><td>list[f64]</td></tr></thead><tbody><tr><td>568703</td><td>269413707</td><td>2023-05-20 07:53:39</td><td>[9768377, 9768387, … 9769893]</td><td>[9772601]</td><td>[9773306, 9773306, … 9772629]</td><td>[0, 0, … 0]</td><td>[2023-05-20 07:41:49, 2023-05-20 07:41:49, … 2023-05-20 06:10:34]</td><td>2023-05-20 07:41:49</td><td>[0.0, 0.0, … 5475.0]</td><td>[1.0, 1.0, … 0.894444]</td></tr><tr><td>1333156</td><td>328878737</td><td>2023-05-21 06:22:19</td><td>[9770178, 9770328, … 9769457]</td><td>[8054212]</td><td>[9569756, 9773857, … 9569756]</td><td>[0, 0, … 0]</td><td>[2023-01-09 06:17:27, 2023-05-21 04:08:55, … 2023-01-09 06:17:27]</td><td>2023-05-21 04:08:55</td><td>[1.1397088e7, 0.0, … 1.1397088e7]</td><td>[0.970995, 1.0, … 0.970995]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 11)\n",
       "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ user_id ┆ impression ┆ impressio ┆ article_i ┆ … ┆ published ┆ reference ┆ time_delt ┆ discount_ │\n",
       "│ ---     ┆ _id        ┆ n_time    ┆ d_fixed   ┆   ┆ _time_art ┆ _date     ┆ a         ┆ time_delt │\n",
       "│ u32     ┆ ---        ┆ ---       ┆ ---       ┆   ┆ icle_ids_ ┆ ---       ┆ ---       ┆ a         │\n",
       "│         ┆ u32        ┆ datetime[ ┆ list[i32] ┆   ┆ inv…      ┆ datetime[ ┆ list[f64] ┆ ---       │\n",
       "│         ┆            ┆ μs]       ┆           ┆   ┆ ---       ┆ μs]       ┆           ┆ list[f64] │\n",
       "│         ┆            ┆           ┆           ┆   ┆ list[date ┆           ┆           ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆ time[μs]] ┆           ┆           ┆           │\n",
       "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 568703  ┆ 269413707  ┆ 2023-05-2 ┆ [9768377, ┆ … ┆ [2023-05- ┆ 2023-05-2 ┆ [0.0,     ┆ [1.0,     │\n",
       "│         ┆            ┆ 0         ┆ 9768387,  ┆   ┆ 20        ┆ 0         ┆ 0.0, …    ┆ 1.0, …    │\n",
       "│         ┆            ┆ 07:53:39  ┆ …         ┆   ┆ 07:41:49, ┆ 07:41:49  ┆ 5475.0]   ┆ 0.894444] │\n",
       "│         ┆            ┆           ┆ 9769893]  ┆   ┆ 2023-05-… ┆           ┆           ┆           │\n",
       "│ 1333156 ┆ 328878737  ┆ 2023-05-2 ┆ [9770178, ┆ … ┆ [2023-01- ┆ 2023-05-2 ┆ [1.139708 ┆ [0.970995 │\n",
       "│         ┆            ┆ 1         ┆ 9770328,  ┆   ┆ 09        ┆ 1         ┆ 8e7, 0.0, ┆ , 1.0, …  │\n",
       "│         ┆            ┆ 06:22:19  ┆ …         ┆   ┆ 06:17:27, ┆ 04:08:55  ┆ … 1.13970 ┆ 0.970995] │\n",
       "│         ┆            ┆           ┆ 9769457]  ┆   ┆ 2023-05-… ┆           ┆ 88…       ┆           │\n",
       "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_train = compute_temporal_differences(\n",
    "    df_train,\n",
    "    f\"published_time_{DEFAULT_INVIEW_ARTICLES_COL}\"\n",
    ")\n",
    "\n",
    "df_validation = compute_temporal_differences(\n",
    "    df_validation,\n",
    "    f\"published_time_{DEFAULT_INVIEW_ARTICLES_COL}\"\n",
    ")\n",
    "\n",
    "df_test = compute_temporal_differences(\n",
    "    df_test,\n",
    "    f\"published_time_{DEFAULT_INVIEW_ARTICLES_COL}\"\n",
    ")\n",
    "\n",
    "df_train = add_time_difference_column(\n",
    "    df_train,\n",
    "    f\"published_time_{DEFAULT_INVIEW_ARTICLES_COL}\",\n",
    "    \"reference_date\", \n",
    "    \"time_delta\"\n",
    ")\n",
    "\n",
    "df_validation = add_time_difference_column(\n",
    "    df_validation,\n",
    "    f\"published_time_{DEFAULT_INVIEW_ARTICLES_COL}\",\n",
    "    \"reference_date\", \n",
    "    \"time_delta\"\n",
    ")\n",
    "\n",
    "df_test = add_time_difference_column(\n",
    "    df_test,\n",
    "    f\"published_time_{DEFAULT_INVIEW_ARTICLES_COL}\",\n",
    "    \"reference_date\", \n",
    "    \"time_delta\"\n",
    ")\n",
    "\n",
    "df_train = add_discount_column(\n",
    "    df_train,\n",
    "    \"time_delta\",\n",
    "    \"discount_time_delta\"\n",
    ")\n",
    "\n",
    "df_validation = add_discount_column(\n",
    "    df_validation,\n",
    "    \"time_delta\",\n",
    "    \"discount_time_delta\"\n",
    ")\n",
    "\n",
    "df_test = add_discount_column(\n",
    "    df_test,\n",
    "    \"time_delta\",\n",
    "    \"discount_time_delta\"\n",
    ")\n",
    "\n",
    "\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init model using HuggingFace's tokenizer and wordembedding\n",
    "In the original implementation, they use the GloVe embeddings and tokenizer. To get going fast, we'll use a multilingual LLM from Hugging Face. \n",
    "Utilizing the tokenizer to tokenize the articles and the word-embedding to init NRMS.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMER_MODEL_NAME = \"FacebookAI/xlm-roberta-base\"\n",
    "TEXT_COLUMNS_TO_USE = [DEFAULT_SUBTITLE_COL, DEFAULT_TITLE_COL]\n",
    "MAX_TITLE_LENGTH = 30\n",
    "\n",
    "# LOAD HUGGINGFACE:\n",
    "transformer_model = AutoModel.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "transformer_tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL_NAME)\n",
    "\n",
    "# We'll init the word embeddings using the\n",
    "word2vec_embedding = get_transformers_word_embeddings(transformer_model)\n",
    "#\n",
    "df_articles, cat_cal = concat_str_columns(df_articles, columns=TEXT_COLUMNS_TO_USE)\n",
    "df_articles, token_col_title = convert_text2encoding_with_transformers(\n",
    "    df_articles, transformer_tokenizer, cat_cal, max_length=MAX_TITLE_LENGTH\n",
    ")\n",
    "# =>\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=token_col_title\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initiate the dataloaders\n",
    "In the implementations we have disconnected the models and data. Hence, you should built a dataloader that fits your needs.\n",
    "\n",
    "Note, with this ```NRMSDataLoader``` the ```eval_mode=False``` is meant for ```model.model.fit()``` whereas ```eval_mode=True``` is meant for ```model.scorer.predict()```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "import tensorflow as tf\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "from ebrec.utils._articles_behaviors import map_list_article_id_to_value\n",
    "from ebrec.utils._python import (\n",
    "    repeat_by_list_values_from_matrix,\n",
    "    create_lookup_objects,\n",
    ")\n",
    "\n",
    "from ebrec.utils._constants import (\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_LABELS_COL,\n",
    "    DEFAULT_USER_COL,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataloader = NRMSTemporalDataLoader(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "val_dataloader = NRMSTemporalDataLoader(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"
     ]
    }
   ],
   "source": [
    "# List all physical devices\n",
    "physical_devices = tf.config.list_physical_devices()\n",
    "print(\"Available devices:\", physical_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate the NRMS-model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janle\\Desktop\\Master_local\\Data_storage\\Deep_learning\\ebnerd_data\\ebnerd_predictions\\state_dict\\NRMSTemporalModel\\mini.weights.h5\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janle\\anaconda3\\envs\\deep_learning\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "c:\\Users\\janle\\anaconda3\\envs\\deep_learning\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_38', 'keras_tensor_39', 'keras_tensor_41']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n",
      "c:\\Users\\janle\\anaconda3\\envs\\deep_learning\\Lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:446: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 192001536 elements. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1260/1260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - AUC: 0.5212 - loss: 2.4627\n",
      "Epoch 1: saving model to C:\\Users\\janle\\Desktop\\Master_local\\Data_storage\\Deep_learning\\ebnerd_data\\ebnerd_predictions\\state_dict\\NRMSTemporalModel\\mini.weights.h5\n",
      "\u001b[1m1260/1260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2779s\u001b[0m 2s/step - AUC: 0.5213 - loss: 2.4624 - val_AUC: 0.5536 - val_loss: 1.7266 - learning_rate: 1.0000e-04\n",
      "Epoch 2/4\n",
      "\u001b[1m1260/1260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - AUC: 0.6022 - loss: 1.5868\n",
      "Epoch 2: saving model to C:\\Users\\janle\\Desktop\\Master_local\\Data_storage\\Deep_learning\\ebnerd_data\\ebnerd_predictions\\state_dict\\NRMSTemporalModel\\mini.weights.h5\n",
      "\u001b[1m1260/1260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2263s\u001b[0m 2s/step - AUC: 0.6022 - loss: 1.5868 - val_AUC: 0.5473 - val_loss: 1.8412 - learning_rate: 1.0000e-04\n",
      "Epoch 3/4\n",
      "\u001b[1m1260/1260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - AUC: 0.6534 - loss: 1.5188\n",
      "Epoch 3: saving model to C:\\Users\\janle\\Desktop\\Master_local\\Data_storage\\Deep_learning\\ebnerd_data\\ebnerd_predictions\\state_dict\\NRMSTemporalModel\\mini.weights.h5\n",
      "\u001b[1m1260/1260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2362s\u001b[0m 2s/step - AUC: 0.6534 - loss: 1.5188 - val_AUC: 0.5609 - val_loss: 1.8776 - learning_rate: 1.0000e-04\n",
      "Epoch 4/4\n",
      "\u001b[1m1260/1260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - AUC: 0.6910 - loss: 1.4565\n",
      "Epoch 4: saving model to C:\\Users\\janle\\Desktop\\Master_local\\Data_storage\\Deep_learning\\ebnerd_data\\ebnerd_predictions\\state_dict\\NRMSTemporalModel\\mini.weights.h5\n",
      "\u001b[1m1260/1260\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2190s\u001b[0m 2s/step - AUC: 0.6910 - loss: 1.4565 - val_AUC: 0.5854 - val_loss: 1.7462 - learning_rate: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "model = NRMSTemporalModel(\n",
    "    hparams=hparams_nrms,\n",
    "    word2vec_embedding=word2vec_embedding,\n",
    "    seed=42,\n",
    ")\n",
    "model.model.compile(\n",
    "    optimizer=model.model.optimizer,\n",
    "    loss=model.model.loss,\n",
    "    metrics=[\"AUC\"],\n",
    ")\n",
    "\n",
    "MODEL_NAME = model.__class__.__name__\n",
    "MODEL_WEIGHTS = DUMP_DIR.joinpath(f\"state_dict/{MODEL_NAME}/mini.weights.h5\")\n",
    "LOG_DIR = DUMP_DIR.joinpath(f\"runs/{MODEL_NAME}\")\n",
    "print(MODEL_WEIGHTS)\n",
    "### Callbacks\n",
    "#We will add some callbacks to model training.\n",
    "# Tensorboard:\n",
    "#tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "#    log_dir=LOG_DIR,\n",
    "#    histogram_freq=1,\n",
    "#)\n",
    "\n",
    "# Earlystopping:\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_AUC\",\n",
    "    mode=\"max\",\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# ModelCheckpoint:\n",
    "modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=MODEL_WEIGHTS,\n",
    "    monitor=\"val_AUC\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=False,\n",
    "    save_weights_only=True,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Learning rate scheduler:\n",
    "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor=\"val_AUC\",\n",
    "    mode=\"max\",\n",
    "    factor=0.2,\n",
    "    patience=2,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, modelcheckpoint, lr_scheduler]#tensorboard_callback\n",
    "USE_CALLBACKS = True\n",
    "EPOCHS = 4\n",
    "\n",
    "hist = model.model.fit(\n",
    "    train_dataloader,\n",
    "    validation_data=val_dataloader,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks if USE_CALLBACKS else [],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_CALLBACKS:\n",
    "    _ = model.model.load_weights(filepath=MODEL_WEIGHTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example how to compute some metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_TEST = 16\n",
    "\n",
    "test_dataloader = NRMSTemporalDataLoader(\n",
    "    behaviors=df_test,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=BATCH_SIZE_TEST,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janle\\anaconda3\\envs\\deep_learning\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_38', 'keras_tensor_40', 'keras_tensor_42']. Received: the structure of inputs=('*', '*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1529/1529\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2973s\u001b[0m 2s/step\n"
     ]
    }
   ],
   "source": [
    "pred_test = model.scorer.predict(test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add the predictions to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 12)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>impression_id</th><th>impression_time</th><th>article_id_fixed</th><th>article_ids_clicked</th><th>article_ids_inview</th><th>labels</th><th>published_time_article_ids_inview</th><th>reference_date</th><th>time_delta</th><th>discount_time_delta</th><th>scores</th></tr><tr><td>u32</td><td>u32</td><td>datetime[μs]</td><td>list[i32]</td><td>list[i32]</td><td>list[i32]</td><td>list[i8]</td><td>list[datetime[μs]]</td><td>datetime[μs]</td><td>list[f64]</td><td>list[f64]</td><td>list[f32]</td></tr></thead><tbody><tr><td>2402163</td><td>215159974</td><td>2023-05-27 03:07:54</td><td>[9779489, 9779538, … 9779867]</td><td>[9783314]</td><td>[9779713, 9783314, … 9193263]</td><td>[0, 1, … 0]</td><td>[2023-05-25 05:51:21, 2023-05-26 19:11:34, … 2022-03-29 12:34:54]</td><td>2023-05-26 19:11:34</td><td>[134413.0, 0.0, … 3.6571e7]</td><td>[0.999082, 1.0, … 0.778801]</td><td>[0.723931, 0.896529, … 0.563379]</td></tr><tr><td>648257</td><td>553921105</td><td>2023-05-30 12:23:38</td><td>[9768997, 9762135, … 9776259]</td><td>[9788352]</td><td>[9788352, 9787769, … 9022428]</td><td>[1, 0, … 0]</td><td>[2023-05-30 11:47:16, 2023-05-30 11:23:40, … 2021-12-04 13:30:35]</td><td>2023-05-30 12:07:25</td><td>[1209.0, 2625.0, … 4.682381e7]</td><td>[0.999994, 0.999986, … 0.778801]</td><td>[0.842986, 0.813382, … 0.519041]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 12)\n",
       "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ user_id ┆ impression ┆ impressio ┆ article_i ┆ … ┆ reference ┆ time_delt ┆ discount_ ┆ scores    │\n",
       "│ ---     ┆ _id        ┆ n_time    ┆ d_fixed   ┆   ┆ _date     ┆ a         ┆ time_delt ┆ ---       │\n",
       "│ u32     ┆ ---        ┆ ---       ┆ ---       ┆   ┆ ---       ┆ ---       ┆ a         ┆ list[f32] │\n",
       "│         ┆ u32        ┆ datetime[ ┆ list[i32] ┆   ┆ datetime[ ┆ list[f64] ┆ ---       ┆           │\n",
       "│         ┆            ┆ μs]       ┆           ┆   ┆ μs]       ┆           ┆ list[f64] ┆           │\n",
       "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 2402163 ┆ 215159974  ┆ 2023-05-2 ┆ [9779489, ┆ … ┆ 2023-05-2 ┆ [134413.0 ┆ [0.999082 ┆ [0.723931 │\n",
       "│         ┆            ┆ 7         ┆ 9779538,  ┆   ┆ 6         ┆ , 0.0, …  ┆ , 1.0, …  ┆ ,         │\n",
       "│         ┆            ┆ 03:07:54  ┆ …         ┆   ┆ 19:11:34  ┆ 3.6571e7] ┆ 0.778801] ┆ 0.896529, │\n",
       "│         ┆            ┆           ┆ 9779867]  ┆   ┆           ┆           ┆           ┆ …         │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆           ┆ 0.56337…  │\n",
       "│ 648257  ┆ 553921105  ┆ 2023-05-3 ┆ [9768997, ┆ … ┆ 2023-05-3 ┆ [1209.0,  ┆ [0.999994 ┆ [0.842986 │\n",
       "│         ┆            ┆ 0         ┆ 9762135,  ┆   ┆ 0         ┆ 2625.0, … ┆ ,         ┆ ,         │\n",
       "│         ┆            ┆ 12:23:38  ┆ …         ┆   ┆ 12:07:25  ┆ 4.682381e ┆ 0.999986, ┆ 0.813382, │\n",
       "│         ┆            ┆           ┆ 9776259]  ┆   ┆           ┆ 7]        ┆ …         ┆ …         │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆ 0.77880…  ┆ 0.51904…  │\n",
       "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = add_prediction_scores(df_test, pred_test)\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AUC: 100%|██████████████████████████████| 24464/24464 [00:16<00:00, 1496.89it/s]\n",
      "AUC: 100%|█████████████████████████████| 24464/24464 [00:00<00:00, 74557.82it/s]\n",
      "AUC: 100%|█████████████████████████████| 24464/24464 [00:00<00:00, 35286.09it/s]\n",
      "AUC: 100%|█████████████████████████████| 24464/24464 [00:00<00:00, 35064.30it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<MetricEvaluator class>: \n",
       " {\n",
       "    \"auc\": 0.5498668756035231,\n",
       "    \"mrr\": 0.3421531055464994,\n",
       "    \"ndcg@5\": 0.3820940871150994,\n",
       "    \"ndcg@10\": 0.4606448696714717\n",
       "}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = MetricEvaluator(\n",
    "    labels=df_test[\"labels\"].to_list(),\n",
    "    predictions=df_test[\"scores\"].to_list(),\n",
    "    metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)],\n",
    ")\n",
    "metrics.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janle\\AppData\\Local\\Temp\\ipykernel_14988\\721020885.py:1: MapWithoutReturnDtypeWarning: Calling `map_elements` without specifying `return_dtype` can lead to unpredictable results. Specify `return_dtype` to silence this warning.\n",
      "  df_test = df_test.with_columns(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 13)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>user_id</th><th>impression_id</th><th>impression_time</th><th>article_id_fixed</th><th>article_ids_clicked</th><th>article_ids_inview</th><th>labels</th><th>published_time_article_ids_inview</th><th>reference_date</th><th>time_delta</th><th>discount_time_delta</th><th>scores</th><th>ranked_scores</th></tr><tr><td>u32</td><td>u32</td><td>datetime[μs]</td><td>list[i32]</td><td>list[i32]</td><td>list[i32]</td><td>list[i8]</td><td>list[datetime[μs]]</td><td>datetime[μs]</td><td>list[f64]</td><td>list[f64]</td><td>list[f32]</td><td>list[i64]</td></tr></thead><tbody><tr><td>2402163</td><td>215159974</td><td>2023-05-27 03:07:54</td><td>[9779489, 9779538, … 9779867]</td><td>[9783314]</td><td>[9779713, 9783314, … 9193263]</td><td>[0, 1, … 0]</td><td>[2023-05-25 05:51:21, 2023-05-26 19:11:34, … 2022-03-29 12:34:54]</td><td>2023-05-26 19:11:34</td><td>[134413.0, 0.0, … 3.6571e7]</td><td>[0.999082, 1.0, … 0.778801]</td><td>[0.723931, 0.896529, … 0.563379]</td><td>[2, 1, … 4]</td></tr><tr><td>648257</td><td>553921105</td><td>2023-05-30 12:23:38</td><td>[9768997, 9762135, … 9776259]</td><td>[9788352]</td><td>[9788352, 9787769, … 9022428]</td><td>[1, 0, … 0]</td><td>[2023-05-30 11:47:16, 2023-05-30 11:23:40, … 2021-12-04 13:30:35]</td><td>2023-05-30 12:07:25</td><td>[1209.0, 2625.0, … 4.682381e7]</td><td>[0.999994, 0.999986, … 0.778801]</td><td>[0.842986, 0.813382, … 0.519041]</td><td>[1, 3, … 5]</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 13)\n",
       "┌─────────┬────────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬───────────┐\n",
       "│ user_id ┆ impression ┆ impressio ┆ article_i ┆ … ┆ time_delt ┆ discount_ ┆ scores    ┆ ranked_sc │\n",
       "│ ---     ┆ _id        ┆ n_time    ┆ d_fixed   ┆   ┆ a         ┆ time_delt ┆ ---       ┆ ores      │\n",
       "│ u32     ┆ ---        ┆ ---       ┆ ---       ┆   ┆ ---       ┆ a         ┆ list[f32] ┆ ---       │\n",
       "│         ┆ u32        ┆ datetime[ ┆ list[i32] ┆   ┆ list[f64] ┆ ---       ┆           ┆ list[i64] │\n",
       "│         ┆            ┆ μs]       ┆           ┆   ┆           ┆ list[f64] ┆           ┆           │\n",
       "╞═════════╪════════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ 2402163 ┆ 215159974  ┆ 2023-05-2 ┆ [9779489, ┆ … ┆ [134413.0 ┆ [0.999082 ┆ [0.723931 ┆ [2, 1, …  │\n",
       "│         ┆            ┆ 7         ┆ 9779538,  ┆   ┆ , 0.0, …  ┆ , 1.0, …  ┆ ,         ┆ 4]        │\n",
       "│         ┆            ┆ 03:07:54  ┆ …         ┆   ┆ 3.6571e7] ┆ 0.778801] ┆ 0.896529, ┆           │\n",
       "│         ┆            ┆           ┆ 9779867]  ┆   ┆           ┆           ┆ …         ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆           ┆ 0.56337…  ┆           │\n",
       "│ 648257  ┆ 553921105  ┆ 2023-05-3 ┆ [9768997, ┆ … ┆ [1209.0,  ┆ [0.999994 ┆ [0.842986 ┆ [1, 3, …  │\n",
       "│         ┆            ┆ 0         ┆ 9762135,  ┆   ┆ 2625.0, … ┆ ,         ┆ ,         ┆ 5]        │\n",
       "│         ┆            ┆ 12:23:38  ┆ …         ┆   ┆ 4.682381e ┆ 0.999986, ┆ 0.813382, ┆           │\n",
       "│         ┆            ┆           ┆ 9776259]  ┆   ┆ 7]        ┆ …         ┆ …         ┆           │\n",
       "│         ┆            ┆           ┆           ┆   ┆           ┆ 0.77880…  ┆ 0.51904…  ┆           │\n",
       "└─────────┴────────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = df_test.with_columns(\n",
    "    pl.col(\"scores\")\n",
    "    .map_elements(lambda x: list(rank_predictions_by_score(x)))\n",
    "    .alias(\"ranked_scores\")\n",
    ")\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is using the validation, simply add the testset to your flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24464it [00:00, 176054.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping C:\\Users\\janle\\Desktop\\Master_local\\Data_storage\\Deep_learning\\ebnerd_data\\ebnerd_predictions\\predictions.txt to C:\\Users\\janle\\Desktop\\Master_local\\Data_storage\\Deep_learning\\ebnerd_data\\ebnerd_predictions\\ebnerd_small_predictions-NRMSTemporalModel.zip\n"
     ]
    }
   ],
   "source": [
    "write_submission_file(\n",
    "    impression_ids=df_test[DEFAULT_IMPRESSION_ID_COL],\n",
    "    prediction_scores=df_test[\"ranked_scores\"],\n",
    "    path=DUMP_DIR.joinpath(\"predictions.txt\"),\n",
    "    filename_zip=f\"{DATASPLIT}_predictions-{MODEL_NAME}.zip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
